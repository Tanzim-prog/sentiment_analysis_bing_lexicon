library(readxl)
library(readxl)
library(tidyverse)
library(tidytext)
library(ggplot2)
data_frame <- read_excel("Pre Processed Dataset.xlsx", sheet = "The Way Dhaka")
View(data_frame)
# Tokenization
tidy_data <- data_frame %>%
unnest_tokens(word, Text)
View(tidy_data)
# Sentiment Lexicon
bing_lexicon <- get_sentiments("bing")
# Sentiment Analysis
sentiment_scores <- tidy_data %>%
inner_join(bing_lexicon, by = "word")
# Summarize
sentiment_summary <- sentiment_scores %>%
group_by(sentiment) %>%
summarize(total_count = n())
# Data Frame
sentiment_df_bing <- as.data.frame(sentiment_summary_bing)
# data frame to xcel
write.xlsx(corpus_df, file = "Dhaka Regency.xlsx", sheetName = "Dhaka Regency", rowNames = FALSE)
library(openxlsx)
library(readxl)
library(stringr)
library(tm)
library(udpipe)
directory <- "D:/AIUB/Thesis/GIT/Dataset"
file_name <- "Hotel Review Data Table.xlsx"
sheet_index <- 1
file_path <- file.path(directory, file_name)
read_data <- read_excel(file_path, sheet = sheet_index)
dataset <- read_data
View(dataset)
# missing values finder
any(is.na(dataset))
clean_data <- na.omit(dataset)
# corpus
corpus <- Corpus(VectorSource(dataset$Review))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeSpecialChars)
library(openxlsx)
library(readxl)
library(stringr)
library(tm)
library(udpipe)
directory <- "D:/AIUB/Thesis/GIT/Dataset"
file_name <- "Hotel Review Data Table.xlsx"
sheet_index <- 1
file_path <- file.path(directory, file_name)
read_data <- read_excel(file_path, sheet = sheet_index)
dataset <- read_data
View(dataset)
# missing values finder
any(is.na(dataset))
clean_data <- na.omit(dataset)
# corpus
corpus <- Corpus(VectorSource(dataset$Review))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeSpecialChars)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, stripWhitespace)
# trimming
corpus$Review <- str_trim(corpus$Review)
# corpus to a data frame
corpus_df <- data.frame(Text = sapply(corpus, as.character), stringsAsFactors = FALSE)
# data frame to xcel
write.xlsx(corpus_df, file = "Dhaka Regency.xlsx", sheetName = "Dhaka Regency", rowNames = FALSE)
library(openxlsx)
library(readxl)
library(stringr)
library(tm)
library(udpipe)
directory <- "D:/AIUB/Thesis/GIT/Dataset"
file_name <- "Hotel Review Data Table.xlsx"
sheet_index <- 1
file_path <- file.path(directory, file_name)
read_data <- read_excel(file_path, sheet = sheet_index)
dataset <- read_data
View(dataset)
# missing values finder
any(is.na(dataset))
clean_data <- na.omit(dataset)
# corpus
corpus <- Corpus(VectorSource(dataset$Review))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeSpecialChars)
library(openxlsx)
library(readxl)
library(stringr)
library(tm)
library(udpipe)
directory <- "D:/AIUB/Thesis/GIT/Dataset"
file_name <- "Hotel Review Data Table.xlsx"
sheet_index <- 1
file_path <- file.path(directory, file_name)
read_data <- read_excel(file_path, sheet = sheet_index)
dataset <- read_data
View(dataset)
# missing values finder
any(is.na(dataset))
clean_data <- na.omit(dataset)
# corpus
corpus <- Corpus(VectorSource(dataset$Review))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeSpecialChars)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, stripWhitespace)
# trimming
corpus$Review <- str_trim(corpus$Review)
# corpus to a data frame
corpus_df <- data.frame(Text = sapply(corpus, as.character), stringsAsFactors = FALSE)
# data frame to xcel
write.xlsx(corpus_df, file = "D:/AIUB/Thesis/GIT/Dataset/Processed Data.xlsx", sheetName = "Dhaka Regency", rowNames = FALSE)
library(readxl)
library(tidyverse)
library(tidytext)
library(ggplot2)
data_frame <- read_excel("D:/AIUB/Thesis/GIT/Dataset/Processed Data.xlsx", sheet = "Dhaka Regency")
View(data_frame)
# Tokenization
tidy_data <- data_frame %>%
unnest_tokens(word, Text)
View(tidy_data)
# Sentiment Lexicon
bing_lexicon <- get_sentiments("bing")
# Sentiment Analysis
sentiment_scores <- tidy_data %>%
inner_join(bing_lexicon, by = "word")
# Summarize
sentiment_summary <- sentiment_scores %>%
group_by(sentiment) %>%
summarize(total_count = n())
# Data Frame
sentiment_df_bing <- as.data.frame(sentiment_summary_bing)
print(sentiment_df_bing)
library(readxl)
library(tidyverse)
library(tidytext)
library(ggplot2)
data_frame <- read_excel("D:/AIUB/Thesis/GIT/Dataset/Processed Data.xlsx", sheet = "Dhaka Regency")
View(data_frame)
# Tokenization
tidy_data <- data_frame %>%
unnest_tokens(word, Text)
View(tidy_data)
# Sentiment Lexicon
bing_lexicon <- get_sentiments("bing")
# Sentiment Analysis
sentiment_scores <- tidy_data %>%
inner_join(bing_lexicon, by = "word")
# Summarize
sentiment_summary_bing <- sentiment_scores %>%
group_by(sentiment) %>%
summarize(total_count = n())
# Data Frame
sentiment_df_bing <- as.data.frame(sentiment_summary_bing)
print(sentiment_df_bing)
# Visualize
ggplot(sentiment_summary_bing, aes(x = sentiment, y = total_count, fill = sentiment)) +
geom_bar(stat = "identity") +
labs(title = "Sentiment Analysis", x = "Sentiment", y = "Total Count") +
theme_minimal()
library(openxlsx)
library(readxl)
library(stringr)
library(tm)
library(udpipe)
directory <- "D:/AIUB/Thesis/GIT/Dataset"
file_name <- "Hotel Review Data Table.xlsx"
sheet_index <- 1
file_path <- file.path(directory, file_name)
read_data <- read_excel(file_path, sheet = sheet_index)
dataset <- read_data
View(dataset)
# missing values finder
any(is.na(dataset))
clean_data <- na.omit(dataset)
#special character removal function
removeSpecialChars <- content_transformer(function(x) gsub("[^[:alnum:][:space:]]", "", x))
# corpus
corpus <- Corpus(VectorSource(dataset$Review))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeSpecialChars)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, stripWhitespace)
# trimming
corpus$Review <- str_trim(corpus$Review)
# corpus to a data frame
corpus_df <- data.frame(Text = sapply(corpus, as.character), stringsAsFactors = FALSE)
# data frame to xcel
write.xlsx(corpus_df, file = "D:/AIUB/Thesis/GIT/Dataset/Processed Data.xlsx", sheetName = "Dhaka Regency", rowNames = FALSE)
library(readxl)
library(tidyverse)
library(tidytext)
library(ggplot2)
data_frame <- read_excel("D:/AIUB/Thesis/GIT/Dataset/Processed Data.xlsx", sheet = "Dhaka Regency")
View(data_frame)
# Tokenization
tidy_data <- data_frame %>%
unnest_tokens(word, Text)
View(tidy_data)
# Sentiment Lexicon
bing_lexicon <- get_sentiments("bing")
# Sentiment Analysis
sentiment_scores <- tidy_data %>%
inner_join(bing_lexicon, by = "word")
# Summarize
sentiment_summary_bing <- sentiment_scores %>%
group_by(sentiment) %>%
summarize(total_count = n())
# Data Frame
sentiment_df_bing <- as.data.frame(sentiment_summary_bing)
print(sentiment_df_bing)
# Visualize
ggplot(sentiment_summary_bing, aes(x = sentiment, y = total_count, fill = sentiment)) +
geom_bar(stat = "identity") +
labs(title = "Sentiment Analysis", x = "Sentiment", y = "Total Count") +
theme_minimal()
library(openxlsx)
library(readxl)
library(stringr)
library(tm)
library(udpipe)
directory <- "D:/AIUB/Thesis/GIT/Dataset"
file_name <- "Hotel Review Data Table.xlsx"
sheet_index <- 1
file_path <- file.path(directory, file_name)
read_data <- read_excel(file_path, sheet = sheet_index)
dataset <- read_data
View(dataset)
# missing values finder
any(is.na(dataset))
clean_data <- na.omit(dataset)
#special character removal function
removeSpecialChars <- content_transformer(function(x) gsub("[^[:alnum:][:space:]]", "", x))
# corpus
corpus <- Corpus(VectorSource(dataset$Review))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeSpecialChars)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, stripWhitespace)
# trimming
corpus$Review <- str_trim(corpus$Review)
# corpus to a data frame
corpus_df <- data.frame(Text = sapply(corpus, as.character), stringsAsFactors = FALSE)
# data frame to xcel
write.xlsx(corpus_df, file = "D:/AIUB/Thesis/GIT/Dataset/Processed Data.xlsx", sheetName = "Dhaka Regency", rowNames = FALSE)
# Download or load a pre-trained udpipe model for English
model_file <- "english-ewt-ud-2.5-191206.udpipe"  # Specify your model file
if (!file.exists(model_file)) {
udpipe_download_model(language = "english")
}
# data frame to xcel
write.xlsx(corpus_df, file = "D:/AIUB/Thesis/GIT/Dataset/Processed Data.xlsx", sheetName = "Dhaka Regency", rowNames = FALSE)
# Load a pre-trained udpipe model for English
model_file <- "D:/AIUB/Thesis/GIT/Udpipe Model/english-ewt-ud-2.5-191206.udpipe"  # Specify your model file
ud_model <- udpipe_load_model(model_file)
# Perform annotation using udpipe model (tokenization, POS tagging, etc.)
udpipe_annotation <- udpipe_annotate(ud_model, x = corpus_df$Text)
annotated_data <- as.data.frame(udpipe_annotation)
#View annotated data (lemmatization, part-of-speech tags, etc.)
View(annotated_data)
#View annotated data (lemmatization, part-of-speech tags, etc.)
View(annotated_data)
# Save the annotated data to an Excel file
write.xlsx(annotated_data, file = "D:/AIUB/Thesis/GIT/Dataset/Annotated Reviews.xlsx", sheetName = "Dhaka Regency", rowNames = FALSE)
library(openxlsx)
library(readxl)
library(udpipe)
directory <- "D:/AIUB/Thesis/GIT/Dataset"
file_name <- "Hotel Review Data Table.xlsx"
sheet_index <- 1
file_path <- file.path(directory, file_name)
read_data <- read_excel(file_path, sheet = sheet_index)
directory <- "D:/AIUB/Thesis/GIT/Dataset"
file_name <- "Processed Data.xlsx"
sheet_index <- 1
file_path <- file.path(directory, file_name)
read_data <- read_excel(file_path, sheet = sheet_index)
dataset <- read_data
View(dataset)
# Load a pre-trained udpipe model for English
model_file <- "D:/AIUB/Thesis/GIT/Udpipe Model/english-ewt-ud-2.5-191206.udpipe"  # Specify your model file
ud_model <- udpipe_load_model(model_file)
# Perform annotation using udpipe model (tokenization, POS tagging, etc.)
udpipe_annotation <- udpipe_annotate(ud_model, x = corpus_df$Text)
annotated_data <- as.data.frame(udpipe_annotation)
#View annotated data (lemmatization, part-of-speech tags, etc.)
View(annotated_data)
# Save the annotated data to an Excel file
write.xlsx(annotated_data, file = "D:/AIUB/Thesis/GIT/Dataset/Annotated Reviews.xlsx", sheetName = "Dhaka Regency", rowNames = FALSE)
#View annotated data (lemmatization, part-of-speech tags, etc.)
View(annotated_data)
View(data_frame)
# Summarize
sentiment_summary_bing <- sentiment_scores %>%
group_by(sentiment) %>%
summarize(total_count = n())
library(readxl)
library(tidyverse)
library(tidytext)
library(ggplot2)
data_frame <- read_excel("D:/AIUB/Thesis/GIT/Dataset/Processed Data.xlsx", sheet = "Dhaka Regency")
View(data_frame)
# Tokenization
tidy_data <- data_frame %>%
unnest_tokens(word, Text)
# Sentiment Lexicon
bing_lexicon <- get_sentiments("bing")
# Sentiment Analysis
sentiment_scores <- tidy_data %>%
inner_join(bing_lexicon, by = "word")
# Summarize
sentiment_summary_bing <- sentiment_scores %>%
group_by(sentiment) %>%
summarize(total_count = n())
# Data Frame
sentiment_df_bing <- as.data.frame(sentiment_summary_bing)
print(sentiment_df_bing)
{# Visualize
ggplot(sentiment_summary_bing, aes(x = sentiment, y = total_count, fill = sentiment)) +
geom_bar(stat = "identity") +
labs(title = "Sentiment Analysis", x = "Sentiment", y = "Total Count") +
theme_minimal()
# Visualize
ggplot(sentiment_summary_bing, aes(x = sentiment, y = total_count, fill = sentiment)) +
geom_bar(stat = "identity") +
labs(title = "Sentiment Analysis", x = "Sentiment", y = "Total Count") +
theme_minimal()
# Visualize
ggplot(sentiment_summary_bing, aes(x = sentiment, y = total_count, fill = sentiment)) +
geom_bar(stat = "identity") +
labs(title = "Sentiment Analysis", x = "Sentiment", y = "Total Count") +
theme_minimal()
library(ggplot2)
# Visualize
ggplot(sentiment_summary_bing, aes(x = sentiment, y = total_count, fill = sentiment)) +
geom_bar(stat = "identity") +
labs(title = "Sentiment Analysis", x = "Sentiment", y = "Total Count") +
theme_minimal()
library(readxl)
library(tidyverse)
library(tidytext)
library(ggplot2)
data_frame <- read_excel("D:/AIUB/Thesis/GIT/Dataset/Processed Data.xlsx", sheet = "Dhaka Regency")
View(data_frame)
# Tokenization
tidy_data <- data_frame %>%
unnest_tokens(word, Text)
View(tidy_data)
library(readxl)
library(readxl)
library(tidyverse)
library(tidytext)
library(ggplot2)
data_frame <- read_excel("D:/AIUB/Thesis/GIT/Dataset/Processed Data.xlsx", sheet = "Dhaka Regency")
View(data_frame)
# Tokenization
tidy_data <- data_frame %>%
unnest_tokens(word, Text)
View(tidy_data)
# Sentiment Lexicon
bing_lexicon <- get_sentiments("bing")
# Sentiment Analysis
sentiment_scores <- tidy_data %>%
inner_join(bing_lexicon, by = "word")
# Summarize
sentiment_summary_bing <- sentiment_scores %>%
group_by(sentiment) %>%
summarize(total_count = n())
# Data Frame
sentiment_df_bing <- as.data.frame(sentiment_summary_bing)
print(sentiment_df_bing)
# Visualize
ggplot(sentiment_summary_bing, aes(x = sentiment, y = total_count, fill = sentiment)) +
geom_bar(stat = "identity") +
labs(title = "Sentiment Analysis", x = "Sentiment", y = "Total Count") +
theme_minimal()
